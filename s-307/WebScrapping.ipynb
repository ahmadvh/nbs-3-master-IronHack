{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Data Hunting and Gathering (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Web Scraping](http://unadocenade.com/wp-content/uploads/2012/09/cavalls-de-valltorta.jpg)\n",
    "\n",
    "Welcome to the first part of our journey into the world of web scraping. Web scraping, also known as web harvesting or web data extraction, is a technique used for extracting data from websites. This process involves fetching the web page and then extracting data from it.\n",
    "\n",
    "### Why Learn Web Scraping?\n",
    "Understanding how to scrape data from the web is a valuable skill for any data professional. In the digital era, data is the new gold, and web scraping is the mining equipment. Here's why it's essential:\n",
    "\n",
    "- **Data Availability**: The internet is a vast source of data for all kinds of analyses, from market trends to academic research.\n",
    "- **Automation**: Web scraping can automate the process of collecting data, saving time and effort.\n",
    "- **Competitive Advantage**: In many fields, having timely and relevant data can be a game-changer.\n",
    "\n",
    "### Real-World Applications\n",
    "- **Market Research**: Analyzing competitors, understanding customer sentiments, and identifying market trends.\n",
    "- **Price Comparison**: Aggregating pricing data from various websites for comparison shopping.\n",
    "- **Social Media Analysis**: Gathering data from social networks for sentiment analysis or trend spotting.\n",
    "\n",
    "### Ethical Considerations in Web Scraping\n",
    "\n",
    "Web scraping, while a powerful technique for data extraction, comes with significant ethical and legal responsibilities. As budding data scientists and web scrapers, it's crucial to navigate this landscape with a deep understanding and respect for these considerations.\n",
    "\n",
    "### Respecting Website Policies and Laws\n",
    "\n",
    "- **Adhering to Terms of Service**: Every website has its own set of rules, usually outlined in its Terms of Service (ToS). It's important to read and understand these rules before scraping, as violating them can have legal implications.\n",
    "\n",
    "- **Following Copyright Laws**: The data you scrape is often copyrighted. Ensure that your use of scraped data complies with copyright laws and respects intellectual property rights.\n",
    "\n",
    "- **Privacy Concerns**: Be mindful of personal data. Scraping and using personal information without consent can breach privacy laws and ethical standards.\n",
    "\n",
    "### Example: Understanding Google's `robots.txt`\n",
    "\n",
    "Google's `robots.txt` file is an excellent example of how websites communicate their scraping policies. Accessible at [Google's robots.txt](https://www.google.com/robots.txt), this file provides directives to web crawlers about which pages they can or cannot scrape.\n",
    "\n",
    "#### Implications of Google's `robots.txt`\n",
    "\n",
    "- **Selective Access**: Google allows certain parts of its site to be crawled while restricting others. For instance, crawling the search results pages is generally disallowed.\n",
    "\n",
    "- **Dynamic Nature**: The content of `robots.txt` files can change, reflecting the website's evolving stance on web scraping. Regular checks are necessary for compliance.\n",
    "\n",
    "- **Respecting the Limits**: Even if a `robots.txt` file allows scraping of some pages, it does not automatically mean all scraping activities are legally or ethically acceptable. It's a guideline, not a blanket permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Data Hunting in the Digital Age\n",
    "\n",
    "#### The Evolution of Data Sourcing\n",
    "\n",
    "In this course, we focus on data as our foundational element. Traditionally, data has been sourced from structured formats like spreadsheets from scientific experiments or records in relational databases within organizations. But with the digital revolution, particularly the advent of the internet, our approach to data collection must evolve. The internet is a vast reservoir of unstructured data, presenting both challenges and opportunities for data retrieval and analysis.\n",
    "\n",
    "#### Understanding the Landscape of Web Data\n",
    "\n",
    "When seeking data from the internet, it's essential to first consider how the website in question provides access to its data. Many large-scale websites like Google, Facebook, and Twitter offer an **Application Programming Interface (API)**. APIs are designed to facilitate easy access to a website's data in a structured format, simplifying the process of data extraction.\n",
    "\n",
    "##### The Role of APIs\n",
    "\n",
    "- **APIs as a Primary Tool**: An API acts as a bridge between the data seeker and the website's database, allowing for streamlined data retrieval.\n",
    "- **Limitations**: However, not all websites provide an API. Additionally, even when an API is available, it may not grant access to all the data a user might need.\n",
    "\n",
    "##### The Need for Web Scraping\n",
    "\n",
    "In cases where an API is absent or insufficient, we turn to **web scraping**. Web scraping involves extracting raw data directly from a website's frontend - essentially, the same information presented to users in their web browsers.\n",
    "\n",
    "###### Diving into Scraping\n",
    "\n",
    "- **Dealing with Unstructured Data**: Scraping requires us to interact with unstructured data, necessitating custom coding and data parsing techniques.\n",
    "- **Legal and Ethical Considerations**: It's crucial to approach web scraping with an awareness of the legal and ethical implications, respecting website policies and user privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Our Journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first practical step in this journey will be to explore how to connect to the internet and retrieve a basic webpage. We'll begin by using Python's `urllib.request` module, a powerful tool for interacting with URLs and handling web requests.\n",
    "\n",
    "Join us as we embark on this exciting journey to master the art of data hunting in the digital era, where we'll navigate the complexities of APIs, web scraping, and the ethical considerations that come with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'urlopen' function from the 'urllib.request' module.\n",
    "# This function is used for opening URLs, which is the first step in web scraping.\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the 'urlopen' function to open the URL 'http://www.google.com/'.\n",
    "# The function returns a response object which can be used to read the content of the page.\n",
    "# Here, 'source' is a variable that holds the response object from the URL.\n",
    "source = urlopen(\"http://www.google.com/\")\n",
    "\n",
    "# Print the response object.\n",
    "# This command does not print the content of the webpage.\n",
    "# Instead, it prints a representation of the response object, \n",
    "# which includes information like the URL, HTTP response status, headers, etc.\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Content Retrieved by `urlopen`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates the basic usage of the `urlopen` function for accessing a webpage. However, it is important to note that `print(source)` will not display the HTML content of the webpage but rather the HTTP response object's representation. To view the actual content of the page, you would need to read from the `source` object using methods like `source.read()`.\n",
    "\n",
    "After opening a URL using the `urlopen` function from the `urllib.request` module, we typically want to access the actual content of the webpage. This is where `source.read()` comes into play.\n",
    "\n",
    "### Understanding `source.read()`\n",
    "\n",
    "When you call `urlopen`, it returns an HTTPResponse object. This object, which we've named `source` in our example, holds various data and metadata about the webpage. To extract the actual HTML content of the page, we use the `read` method on this object.\n",
    "\n",
    "### What Does `source.read()` Do?\n",
    "\n",
    "- **Retrieves Webpage Content**: `source.read()` reads the entire content of the webpage to which the URL points. This content is usually in HTML format, which is the standard language for creating webpages.\n",
    "\n",
    "- **Binary Format**: The data retrieved is in binary format. To work with it as a string in Python, you might need to decode it using a method like `.decode('utf-8')`.\n",
    "\n",
    "- **One-time Operation**: It's important to note that you can read the content of the response only once. After `source.read()` is executed, the response object does not retain the content in a readable form. If you need to access the content again, you must reopen the URL.\n",
    "\n",
    "Here's a simple example to illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check what is in\n",
    "something = source.read()\n",
    "print(something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our hands-on with some initial exercises to get warmed up with web scraping!\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. **Python.org Content Check**: Does [https://www.python.org](https://www.python.org) contain the word `Python`?  \n",
    "   _Hint: You can use the `in` keyword to check._\n",
    "\n",
    "2. **Google.com Image Search**: Does [http://google.com](http://google.com) contain an image?  \n",
    "   _Hint: Look for the `<img>` tag._\n",
    "\n",
    "3. **First Characters of Python.org**: What are the first ten characters of [https://www.python.org](https://www.python.org)?\n",
    "\n",
    "4. **Keyword Check in Pyladies.com**: Is there the word 'python' in [https://pyladies.com](https://pyladies.com)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX1: Check if 'Python' is in the content of http://www.python.org/\n",
    "\n",
    "# Import the urlopen function from the urllib.request module\n",
    "# This function is used to open a URL and retrieve its contents\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the urlopen function to access the webpage at http://www.python.org/\n",
    "# The function returns an HTTPResponse object which is stored in the variable 'source'\n",
    "source = urlopen(\"http://www.python.org/\")\n",
    "\n",
    "# Read the content of the response object using the read() method\n",
    "# The read() method retrieves the content of the webpage in binary format\n",
    "# The binary content is then decoded to a string using the 'latin-1' encoding\n",
    "# The decoded string is stored in the variable 'something'\n",
    "something = source.read().decode('latin-1')\n",
    "\n",
    "# Check if the word \"Python\" is in the decoded string\n",
    "# This is done using the 'in' keyword, which checks for the presence of a substring in a string\n",
    "# The result is a boolean value: True if \"Python\" is found, False otherwise\n",
    "\"Python\" in something\n",
    "\n",
    "# Note: The choice of 'latin-1' for decoding might not always be appropriate\n",
    "# It's often better to use 'utf-8', which is a more common encoding for webpages\n",
    "# For example: something = source.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions: Request, Crawling and Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `urlopen` vs. `Request` in Web Scraping\n",
    "\n",
    "When performing web scraping tasks in Python, you have the option to use either the `urlopen` function from the `urllib.request` module or the `Request` object in combination with `urlopen`. Here, we'll explain why you might choose one approach over the other.\n",
    "\n",
    "### Using `urlopen` Directly\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- **Simplicity**: It's a straightforward way to access a webpage and retrieve its content without the need for additional objects or customization.\n",
    "  \n",
    "- **Default Behavior**: `urlopen` uses default settings for the HTTP request, which is suitable for many common use cases.\n",
    "\n",
    "- **Convenience**: For simple web scraping tasks, it provides a concise and readable solution.\n",
    "\n",
    "### Using `Request` with `urlopen`\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- **Customization**: You can set custom headers, use different HTTP methods (e.g., POST, PUT), and configure advanced options like handling redirects, cookies, and timeouts.\n",
    "\n",
    "- **Fine-Grained Control**: It offers greater flexibility for handling complex scenarios.\n",
    "\n",
    "In summary, the choice between using `urlopen` directly and creating a `Request` object depends on the complexity of your web scraping task. For simple tasks like fetching webpage content, `urlopen` is often sufficient and more straightforward. However, if you need to customize headers, use non-GET HTTP methods, or handle advanced scenarios, creating a `Request` object allows for fine-grained control over your HTTP requests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling and Scraping: Unveiling the Web's Secrets\n",
    "\n",
    "Crawling and scraping are two fundamental techniques in the world of web data acquisition. They form the backbone of many data-driven applications and are crucial skills for data analysts and web developers.\n",
    "\n",
    "### Crawling: Navigating the Web\n",
    "\n",
    "Crawling, often referred to as web crawling or web scraping, is the process of systematically navigating the World Wide Web to retrieve web pages. Think of it as a web robot or spider, tirelessly traversing the internet to discover and index web content. This technique is at the heart of search engines like Google and Bing.\n",
    "\n",
    "### Why Do We Crawl?\n",
    "\n",
    "Crawling serves several important purposes:\n",
    "\n",
    "- **Indexing**: It allows search engines to index and catalog web pages, making them searchable by users.\n",
    "  \n",
    "- **Link Discovery**: Crawlers extract links from web pages, helping build a vast network of interconnected web resources. This link structure is crucial for understanding the web's architecture.\n",
    "  \n",
    "- **Data Retrieval**: Crawlers may scrape or extract data from web pages, but their primary goal is to discover and navigate to other web pages.\n",
    "\n",
    "### Scraping: Harvesting Data\n",
    "\n",
    "Scraping is the process of extracting specific data or information from a single web page. Unlike crawling, which focuses on navigating the web, scraping zooms in on a single webpage to harvest valuable data.\n",
    "\n",
    "### Use Cases of Scraping\n",
    "\n",
    "Scraping is used for a variety of purposes, such as:\n",
    "\n",
    "- **Data Extraction**: It allows us to extract structured data like product prices, news headlines, or stock market information from websites.\n",
    "\n",
    "- **Content Monitoring**: Scraping can be employed to track changes in content on specific web pages, such as monitoring price changes on e-commerce sites or tracking news updates.\n",
    "\n",
    "- **Competitor Analysis**: Businesses often use scraping to gather data on competitors, such as pricing strategies or product listings.\n",
    "\n",
    "- **Research and Analysis**: Data analysts and researchers use scraping to collect data for studies, reports, and data-driven insights.\n",
    "\n",
    "### Crawling and Scraping Synergy\n",
    "\n",
    "In practice, crawling and scraping often work together. Crawlers traverse the web to find new pages, and once they reach a page of interest, scraping techniques are applied to extract valuable data. This synergy is what powers search engines, news aggregators, and data-driven applications on the internet.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Understanding the concepts of crawling and scraping is essential for anyone looking to work with web data. Whether you want to build a search engine, gather market research, or simply automate data collection, these techniques are your gateway to unlocking the wealth of information available on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests vs Urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Url Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.pyladies.com'\n",
    "\n",
    "# Set up the request with a custom user-agent header\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Magic Browser'})\n",
    "\n",
    "# Open the URL and retrieve the HTML content\n",
    "con = urllib.request.urlopen(req)\n",
    "html = con.read().decode()\n",
    "\n",
    "# Check if 'Python' is in the HTML content\n",
    "print('Python' in html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main library you will need for webscraping is called Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "# the second package we will need we already know it\n",
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Marie_Curie\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HTTPStatus](https://www.whatismyip.com/static/51e6afd43d8a39f7a6e03805c1328e11/https-codes.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALYZE THE RESPONSE METHODS\n",
    "#response.\n",
    "response.content\n",
    "\n",
    "## This is not very easy to analyze..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the response into a beautiful soup object\n",
    "soup = BeautifulSoup(response.content)\n",
    "# prettify the soup to then copy it to a text editor and study its structure\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BREAK: Html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \"Making Your Own API\": Web Scraping\n",
    "\n",
    "### Understanding Web Scraping\n",
    "Web scraping becomes essential when data is available on the web but isn't accessible through an API, or the existing API lacks certain functionalities or has restrictive terms of service. In such scenarios, **Web Scraping** is the technique that enables automated extraction of this data, replicating the access a human would have visually.\n",
    "\n",
    "### Why Web Scraping?\n",
    "- **Data Accessibility**: Sometimes, the only way to access certain data is directly from the web pages where it is displayed.\n",
    "- **Flexibility**: Web scraping allows you to tailor data extraction to specific needs, bypassing limitations of existing APIs.\n",
    "\n",
    "### Preparing for Web Scraping: Understanding Web Page Structure\n",
    "Before delving into scraping, it's crucial to have a basic understanding of web page structure and how data is stored and presented. This session covers:\n",
    "\n",
    "#### Basic HTML and CSS Static Pages\n",
    "- **HTML (HyperText Markup Language)**: The standard markup language used to create web pages. Understanding HTML is key to identifying the data you want to scrape.\n",
    "- **CSS (Cascading Style Sheets)**: Used for describing the presentation of a document written in HTML. Knowing CSS helps in pinpointing specific elements on a page.\n",
    "\n",
    "#### Dynamic HTML\n",
    "- **Basic JavaScript Example Using JQuery**: Websites often use JavaScript to load data dynamically. Understanding how this works is crucial for scraping data from such dynamic pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Foundation of Web Pages\n",
    "\n",
    "The most fundamental web pages are constructed using HTML and CSS. These technologies serve two primary purposes: **HTML (Hypertext Markup Language)** structures and stores the content, making it the primary target for web scraping, while **CSS (Cascading Style Sheets)** formats and styles the content, highlighting visual elements like fonts, colors, borders, and layout.\n",
    "\n",
    "#### HTML: The Structure of the Web\n",
    "HTML is a markup language typically rendered by web browsers. It uses 'tags' to define elements on a web page. A typical tag format includes a tag name, attributes (if any), and the content between opening and closing tags.\n",
    "\n",
    "#### Key Components of an HTML File\n",
    "\n",
    "- **DOCTYPE Declaration**: \n",
    "  - Begins with `<!DOCTYPE html>`, indicating the use of HTML5.\n",
    "  - Earlier HTML versions had different DOCTYPEs.\n",
    "\n",
    "- **HTML Tag**: \n",
    "  - The `html` tag (and its closing `/html` tag) encloses the entire web page content.\n",
    "\n",
    "- **Head and Body**: \n",
    "  - The `head` section often includes the `title` tag, defining the webpage's name, links to CSS stylesheets, and JavaScript files for dynamic behavior.\n",
    "  - The `body` contains the visible webpage content.\n",
    "\n",
    "- **Common HTML Elements**:\n",
    "  - **Headings and Paragraphs**: Use `h#` (where # is a number) for headings and `p` for paragraphs.\n",
    "  - **Hyperlinks**: Defined with the `href` attribute in `a` (anchor) tags.\n",
    "  - **Images**: Embedded using `img` tags with the `src` attribute. Note: `img` is self-closing.\n",
    "\n",
    "#### Exercise: Build a Basic HTML Web Page\n",
    "\n",
    "Let's put your HTML knowledge into practice:\n",
    "\n",
    "- Create a file named 'example.html' in your favorite text editor.\n",
    "- Build a basic HTML web page containing elements like `title`, `h1`, `p`, `img`, and `a` tags. Remember that nearly all tags need to be closed with a `/tag`.\n",
    "\n",
    "This exercise aims to familiarize you with the basic structure of HTML and how various elements come together to form a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<!-- Start of the HTML head section -->\n",
    "<head>\n",
    "    <!-- Title of the webpage -->\n",
    "    <title>\n",
    "        Basic knowledge for web scraping.\n",
    "    </title>\t\n",
    "</head>\n",
    "<!-- Start of the HTML body section -->\n",
    "<body>\n",
    "    <!-- Header 1 indicating the subject of the content -->\n",
    "    <h1>About HTML\n",
    "    </h1>\n",
    "\n",
    "    <!-- Image of a rubber ducky; this one is not clickable -->\n",
    "    <p>\n",
    "\n",
    "    </p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<!-- Start of the HTML head section -->\n",
    "<head>\n",
    "    <!-- Title of the webpage -->\n",
    "    <title>\n",
    "        Basic knowledge for web scraping.\n",
    "    </title>\t\n",
    "</head>\n",
    "<!-- Start of the HTML body section -->\n",
    "<body>\n",
    "    <!-- Header 1 indicating the subject of the content -->\n",
    "    <h1>About HTML\n",
    "    </h1>\n",
    "    <!-- Paragraph explaining what HTML is and providing a link for further information -->\n",
    "    <p>Html (Hypertext markdown language) is the basic language to provide contents in the web. It is a tagged language. You can check more about it in <a href=\"http://www.w3.org/community/webed/wiki/HTML\">World Wide Web Consortium.</a></p>\n",
    "    \n",
    "    <!-- Paragraph indicating that one of the following images is clickable -->\n",
    "    <p> One of the following rubberduckies is clickable\n",
    "    </p>\n",
    "    <!-- Image of a rubber ducky; this one is not clickable -->\n",
    "    <p>\n",
    "        <img src = \"files/rubberduck.jpg\"/>\n",
    "    \n",
    "        <!-- Clickable image (hyperlinked) of a rubber ducky -->\n",
    "        <a href=\"http://www.pinterest.com/misscannabliss/rubber-duck-mania/\"><img src = \"files/rubberduck.jpg\"/></a>\n",
    "    </p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Requests and Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles, Paragraphs and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have the html code inside a soup object -> we can explore it's attributes\n",
    "# I can call the title tag of the webpage -> this brings the tag and the content\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine you only wanted the content\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine I want paragraphs (p tag)\n",
    "soup.p\n",
    "# this is no good, clearly there are many p tags which we want\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "paragraphs\n",
    "\n",
    "for element in paragraphs:\n",
    "  print(element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can search both by the tag but also by other attributes, such as the class name\n",
    "tables = soup.find_all('table', attrs= {'class' : 'infobox biography vcard'})\n",
    "\n",
    "#this is very helpful to identify boxes that use the same css styling, for which an attrivute is already defined\n",
    "\n",
    "# finds all the text elements inside the table\n",
    "table = tables[0]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inside the first level of my table, there are still many many tags\n",
    "# you can find more tags within your table\n",
    "\n",
    "# the table itself has many tags inside -> it is a soup object itself\n",
    "for line in table.find_all('li'):\n",
    "  print(line.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it yourself:\n",
    "# find all the bio fields category names for Mdme Curie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Exercise: Extracting News Headlines from BBC Technology\n",
    "\n",
    "#### Objective\n",
    "Write a Python script to scrape headlines from BBC's Technology news section and categorize them based on keywords.\n",
    "\n",
    "#### Task Details\n",
    "\n",
    "1. **Website to Scrape**:\n",
    "   - Target the BBC's 'Technology' section: [BBC Technology News](https://www.bbc.co.uk/news/technology).\n",
    "\n",
    "2. **Scraping Requirement**:\n",
    "   - Scrape the main headlines from the page, typically found in `h3` tags or a specific class.\n",
    "\n",
    "3. **Categorization**:\n",
    "   - Categorize the headlines based on predefined keywords like 'Apple', 'Microsoft', 'Google', etc.\n",
    "   - Count the number of headlines that fall into each category.\n",
    "\n",
    "4. **Output**:\n",
    "   - Print each headline along with its respective category.\n",
    "   - Summarize with the count of headlines in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC technology news section\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Define categories and associated keywords\n",
    "categories = {\n",
    "    'Apple': ['Apple', 'iPhone', 'iPad'],\n",
    "    'Microsoft': ['Microsoft', 'Windows', 'Bill Gates'],\n",
    "    'Google': ['Google', 'Android', 'Alphabet']\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Function to determine the category of a headline\n",
    "def categorize_headline(headline):\n",
    "    # Logic to determine the category based on keywords\n",
    "    # Return the category name if a keyword is found, else return 'Other'\n",
    "    pass\n",
    "\n",
    "# Scrape and process the headlines\n",
    "# Look for 'h3' tags or other relevant tags\n",
    "# Use the categorize_headline function to categorize each headline\n",
    "# Print each headline and its category\n",
    "\n",
    "# Print the count of headlines in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work: Alternatices like Selenium and XPath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Selecting Elements with XPath\n",
    "\n",
    "XPath, or XML Path Language, is a versatile and robust tool for navigating and selecting elements within HTML documents. While Beautiful Soup and requests are commonly used libraries for web scraping, XPath offers a unique and powerful approach to extracting data from web pages.\n",
    "\n",
    "### What is XPath?\n",
    "\n",
    "XPath was originally designed for navigating XML documents, but it is equally applicable to HTML, which shares a structural similarity with XML. XPath allows you to specify the precise location of elements or data within an HTML document using a concise and expressive syntax.\n",
    "\n",
    "### Key Differentiators:\n",
    "\n",
    "Here are some key differentiators that set XPath apart from other web scraping approaches:\n",
    "\n",
    "1. **Granular Selection**: XPath provides granular control over element selection. Unlike Beautiful Soup, which often requires multiple iterations and filtering, XPath allows you to pinpoint elements directly based on their attributes, tags, or positions within the document.\n",
    "\n",
    "2. **Hierarchical Navigation**: XPath excels at navigating the hierarchical structure of HTML documents. It enables you to traverse the document tree, moving up, down, or across branches with ease.\n",
    "\n",
    "3. **Precise Queries**: With XPath, you can create precise queries to extract specific data. For example, you can target elements with specific attributes, such as selecting all `<a>` elements with a particular class or locating elements within specific parent elements.\n",
    "\n",
    "4. **Text Extraction**: XPath's `text()` function simplifies the extraction of text content from elements. This is particularly useful for scraping text data, such as headlines, paragraphs, or product descriptions.\n",
    "\n",
    "### How to Use XPath:\n",
    "\n",
    "To utilize XPath for web scraping, you typically follow these steps:\n",
    "\n",
    "1. **Send an HTTP Request**: Use a library like requests to send an HTTP GET request to the webpage you want to scrape. This retrieves the HTML content of the page.\n",
    "\n",
    "2. **Parse the HTML**: Once you have the HTML content, parse it using a library like lxml or lxml.html. This step constructs a structured representation of the webpage that you can navigate with XPath.\n",
    "\n",
    "3. **Construct XPath Expressions**: Formulate XPath expressions that target the specific elements or data you wish to extract. XPath expressions can vary in complexity, allowing you to adapt to different webpage structures.\n",
    "\n",
    "4. **Apply XPath Expressions**: Apply your XPath expressions to the parsed HTML document to select the desired elements or data. This process effectively filters the HTML content to capture only what you need.\n",
    "\n",
    "5. **Retrieve and Process Data**: Retrieve the selected elements or data using the XPath queries and process them as needed for your scraping task.\n",
    "\n",
    "In summary, XPath is a powerful tool for web scraping that offers precise and efficient element selection within HTML documents. While libraries like Beautiful Soup and requests are valuable, XPath provides an additional layer of control and flexibility, making it a valuable choice for advanced scraping projects.\n",
    "\n",
    "\n",
    "### Understanding XPath Syntax\n",
    "\n",
    "- **Absolute Path (`/`)**: \n",
    "  - Using a single slash indicates an absolute path from the root element.\n",
    "  - Example: `xpath('/html/body/p')` selects all paragraph (`<p>`) elements directly under the `<body>` within the `<html>` root element.\n",
    "\n",
    "- **Relative Path (`//`)**:\n",
    "  - Double slashes indicate a relative path, meaning the selection can start anywhere in the document hierarchy.\n",
    "  - Example: `xpath('//a/div')` finds all `<div>` elements that are descendants of `<a>` tags, regardless of their specific location in the document.\n",
    "\n",
    "- **Wildcards (`*`)**:\n",
    "  - The asterisk acts as a wildcard, representing any element.\n",
    "  - Example: `xpath('//a/div/*')` selects all elements that are children of `<div>` tags under `<a>` tags, anywhere in the document.\n",
    "  - Another example: `xpath('/*/*/div')` finds `<div>` elements that are at the second level of the hierarchy from the root.\n",
    "\n",
    "- **Selecting Specific Elements (Using Brackets)**:\n",
    "  - If a selection returns multiple elements, you can specify which one to select using brackets.\n",
    "  - Example: `xpath('//a/div[1]')` selects the first `<div>` in the set; `xpath('//a/div[last()]')` selects the last `<div>`.\n",
    "\n",
    "### Working with Attributes\n",
    "\n",
    "- **Selecting Attributes (`@`)**:\n",
    "  - The `@` symbol is used to work with element attributes.\n",
    "  - Example: `xpath('//@name')` selects all attributes named 'name' in the document.\n",
    "  - To select `<div>` elements with a 'name' attribute: `xpath('//div[@name]')`.\n",
    "  - To select `<div>` elements without any attributes: `xpath('//div[not(@*)]')`.\n",
    "  - To find `<div>` elements with a specific 'name' attribute value: `xpath('//div[@name=\"chachiname\"]')`.\n",
    "\n",
    "### Utilizing Built-in Functions\n",
    "\n",
    "- XPath comes with several built-in functions to aid in element selection.\n",
    "  - `contains()`: Selects elements containing a specific substring. Example: `xpath('//*[contains(name(),'iv')]')`.\n",
    "  - `count()`: Used for conditional selection based on child count. Example: `xpath('//*[count(div)=2]')`.\n",
    "\n",
    "### Combining Paths and Selecting Relatives\n",
    "\n",
    "- **Combining Paths (`|`)**:\n",
    "  - Use the pipe symbol to combine paths, functioning like an OR operator.\n",
    "  - Example: `xpath('/div/p|/div/a')` selects elements matching either `div/p` or `div/a`.\n",
    "\n",
    "- **Selecting Relatives**:\n",
    "  - You can refer to various relational aspects like parent, ancestors, children, or descendants.\n",
    "  - Example: `xpath('//div/div/parent::*')` selects the parent elements of `div/div` paths.\n",
    "\n",
    "Understanding XPath is essential for effective web scraping, as it allows precise targeting and extraction of data based on the structure of a webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Starting with Selenium \n",
    "\n",
    "Selenium is a powerful tool primarily used for automating web browsers. It's widely utilized in areas such as web scraping, automated testing, and automating web-based administration tasks.\n",
    "\n",
    "### Introduction to Selenium Without Geckodriver\n",
    "\n",
    "Traditionally, Selenium works in conjunction with a driver specific to each browser, like geckodriver for Firefox or chromedriver for Chrome. However, recent developments have enabled certain browsers to be controlled directly by Selenium without the need for an additional driver:\n",
    "\n",
    "- **Chrome**: Recent versions of Google Chrome can be controlled by Selenium directly through the Chrome DevTools Protocol. This simplifies the setup process as you don't need to download and set up chromedriver separately.\n",
    "\n",
    "- **Microsoft Edge**: Similar to Chrome, the Edge browser (Chromium version) can also be automated directly using Selenium with its built-in driver capabilities. \n",
    "\n",
    "This approach of using Selenium without an additional driver streamlines browser automation tasks, making it more accessible and easier to configure, especially for beginners and those looking to quickly set up automated browser interactions.\n",
    "\n",
    "### 2.1 Basic Concepts of Selenium WebDriver\n",
    "\n",
    "### Understanding WebDriver\n",
    "\n",
    "WebDriver is a key component of the Selenium suite. It acts as an interface to interact with the web browser, allowing you to control it programmatically. WebDriver can perform operations like opening web pages, clicking buttons, entering text in forms, and extracting data from web pages.\n",
    "\n",
    "#### Key Functions of WebDriver\n",
    "- **Opening a Web Page**: WebDriver can navigate to a specific URL.\n",
    "- **Locating Elements**: It can find elements on a web page based on their attributes (like ID, name, XPath).\n",
    "- **Interacting with Elements**: WebDriver can simulate actions like clicking buttons, typing text, and submitting forms.\n",
    "\n",
    "### Interacting with Web Elements\n",
    "\n",
    "You can locate and interact with elements on a web page using various methods provided by WebDriver. The choice of method depends on the attributes of the HTML elements you're targeting.\n",
    "\n",
    "- **find_element_by_id**: Locates an element by its unique ID.\n",
    "- **find_element_by_name**: Finds an element by its name attribute.\n",
    "- **find_element_by_xpath**: Uses XPath queries to locate elements, providing a powerful way to navigate the DOM.\n",
    "\n",
    "```python\n",
    "### Selenium WebDriver Python Examples\n",
    "#### Example 1: Opening a Web Page\n",
    "\n",
    "#This example demonstrates how to open a web page using Selenium WebDriver.\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open a web page\n",
    "driver.get(\"https://www.python.org\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIS (Aplication Programing Interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "#### URLs\n",
    "\n",
    "**U**niform **R**esource **L**ocator\n",
    "\n",
    "Contains the information about a resource we (the CLIENT) are requesting from a SERVER\n",
    "\n",
    "http://www.google.com/search?q=puppies\n",
    "\n",
    "http://127.0.0.1:306/invocations\n",
    "\n",
    "- Protocol: http\n",
    "- Top Level Domain: com\n",
    "- Domain: google\n",
    "- Subdomain: www\n",
    "- IP: 127.0.0.1\n",
    "- Port: 306\n",
    "- Route/Folder/Path: search/invocations\n",
    "- Query Parameters: q=puppies\n",
    "\n",
    "#### HTTP\n",
    "**H**yper **T**ext **T**transfer **P**rotocol (**S**ecure)   \n",
    "\n",
    "HTTP(S) is a protocol that provides a structure for request between a client and a server.\n",
    "For example, the web browser of a user (the client) uses HTTP to request information from a server that hoist a website\n",
    "\n",
    "#### Response\n",
    "The response is usually dependent on the functionality you are looking for:\n",
    " * a JSON  \n",
    " * an image\n",
    " * a video\n",
    " * an HTML\n",
    " * ...\n",
    "\n",
    "#### Request\n",
    "**Requests** are the questions we (clients) ask of a server to get some information (the **response**).        \n",
    "Types of request (verbs):\n",
    " * GET: read info from a resource and do not change it in any way. This is the standard request that in most sites gets the HTML+CSS of the page as a response.\n",
    " * POST: send data that creates/updates a resource, or triggers some process.\n",
    " * PUT\n",
    " * DELETE\n",
    " * PATCH\n",
    " * ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFL = requests.get('https://api.tfl.gov.uk/AirQuality')\n",
    "\n",
    "TFL.headers\n",
    "TFL.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Content is typically in a JSON format - What does a json look like?\n",
    "TFL.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![json](https://www.convertsimple.com/wp-content/uploads/2022/05/json-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onboarding API data into Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weather_data = pd.DataFrame.from_dict(TFL.json())\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not ideal, part of the request is still in json. This is a nested json...\n",
    "weather_data['currentForecast'][1]\n",
    "\n",
    "# There is a function in pandas to un-nest jsons, but it makes some assumptions and sometimes we have to unpack hierarchical structures ourselves\n",
    "# beware this usually involves a lot of for loops and apply functions\n",
    "pd.json_normalize(weather_data['currentForecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://v2.jokeapi.dev/joke/programming')\n",
    "r.json()\n",
    "\n",
    "\n",
    "# Sometimes we want to pass parameters to the endpoint, just like we pass arguments to functions in python  \n",
    "# We pass parameters via the url as `?param1=value1&param2=value2...` at the end of the url\n",
    "r = requests.get('https://v2.jokeapi.dev/joke/programming?contains=python&amount=3')\n",
    "r.json()\n",
    "\n",
    "params_dict = {\"contains\":\"python\",\"amount\":\"3\"}\n",
    "r = requests.get('https://v2.jokeapi.dev/joke/programming',params=params_dict)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Challange API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Fetching Weather Data Using OpenWeatherMap API in Python\n",
    "\n",
    "This example demonstrates how to use the OpenWeatherMap API to fetch current weather data for a specific city using Python.\n",
    "\n",
    "#### Prerequisites\n",
    "- An API key from OpenWeatherMap.\n",
    "- Python's `requests` library installed. (Install via `pip install requests` if needed.)\n",
    "\n",
    "#### Steps to Follow\n",
    "1. **Sign Up for OpenWeatherMap API**:\n",
    "   - Register for an account at [OpenWeatherMap](https://openweathermap.org/api).\n",
    "   - Obtain your free API key (note that there might be an activation delay).\n",
    "\n",
    "2. **Python Script for Weather Data Retrieval**:\n",
    "   - The script uses the `requests` library to make an API call.\n",
    "   - Replace `'YOUR_API_KEY'` with your actual OpenWeatherMap API key.\n",
    "   - Replace `'CITY_NAME'` with your desired city name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Challenge II - Optional\n",
    "\n",
    "#### Challenge: Analyzing Instagram Hashtag Trends with Instaloader\n",
    "\n",
    "## Objective\n",
    "Leverage `Instaloader`, a Python library, to download posts associated with a specific hashtag on Instagram. Analyze the collected data to identify trends, popular content, and user engagement.\n",
    "\n",
    "## Steps\n",
    "\n",
    "### 1. Install Instaloader\n",
    "- Ensure Python is installed on your system.\n",
    "- Install `Instaloader` using pip: `pip install instaloader`\n",
    "\n",
    "\n",
    "### 2. Data Collection\n",
    "- Choose a hashtag relevant to a topic of interest (e.g., #nature, #travel, #food).\n",
    "- Use `Instaloader` to download posts tagged with the chosen hashtag. Consider limitations like the number of posts to avoid overwhelming the API.\n",
    "\n",
    "```python\n",
    "import instaloader\n",
    "\n",
    "L = instaloader.Instaloader()\n",
    "posts = instaloader.Hashtag.from_name(L.context, 'YOUR_HASHTAG').get_posts()\n",
    "\n",
    "for post in posts:\n",
    "    # Add code to process and store post details\n",
    "```\n",
    "### 3. Data Analysis\n",
    "Analyze the downloaded data for:\n",
    "- Popular trends in the hashtag.\n",
    "- Common themes or subjects in images or captions.\n",
    "- Levels of user engagement (likes, comments).\n",
    "\n",
    "### 4. Reporting\n",
    "- Compile your findings into a report.\n",
    "- Include visual representations (graphs, word clouds) to illustrate key trends.\n",
    "\n",
    "### Important Notes\n",
    "- Respect Instagram's terms of service and ethical guidelines in data scraping.\n",
    "- Be mindful of privacy and consent, especially with user-generated content.\n",
    "- The scope of data collection should be limited for educational purposes.\n",
    "\n",
    "### Expected Outcome\n",
    "This challenge aims to provide practical experience with Instaloader, develop data analysis skills, and offer insights into social media trends and user behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Marie_Curie\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content)\n",
    "tables = soup.find_all('table', attrs= {'class' : 'infobox biography vcard'})\n",
    "for line in tables[0].find_all('th'):\n",
    "  print(line.text)\n",
    "\n",
    "for line in tables[0].find_all('td'):\n",
    "  print(line.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC technology news section\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Define categories and associated keywords\n",
    "categories = {\n",
    "    'Apple': ['Apple', 'iPhone', 'iPad'],\n",
    "    'Microsoft': ['Microsoft', 'Windows', 'Bill Gates'],\n",
    "    'Google': ['Google', 'Android', 'Alphabet']\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Function to determine the category of a headline\n",
    "def categorize_headline(headline):\n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in headline:\n",
    "                return category\n",
    "    return 'Other'\n",
    "\n",
    "# Scrape and process the headlines\n",
    "# Look for 'h3' tags or other relevant tags\n",
    "headlines = soup.find_all('h3')\n",
    "category_counts = {category: 0 for category in categories.keys()}\n",
    "category_counts['Other'] = 0\n",
    "\n",
    "for h in headlines:\n",
    "    headline_text = h.get_text().strip()\n",
    "    category = categorize_headline(headline_text)\n",
    "    category_counts[category] += 1\n",
    "    print(f\"Headline: {headline_text}\\nCategory: {category}\\n\")\n",
    "\n",
    "# Print the count of headlines in each category\n",
    "print(\"Headline Counts by Category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(api_key, city):\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "    city_name = city\n",
    "    complete_url = f\"{base_url}appid={api_key}&q={city_name}\"\n",
    "    response = requests.get(complete_url)\n",
    "    return response.json()\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual API key and 'CITY_NAME' with your city\n",
    "api_key = 'YOUR_API_KEY'\n",
    "city_name = 'CITY_NAME'\n",
    "weather_data = get_weather(api_key, city_name)\n",
    "\n",
    "print(f\"Weather in {city_name}:\")\n",
    "print(weather_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
